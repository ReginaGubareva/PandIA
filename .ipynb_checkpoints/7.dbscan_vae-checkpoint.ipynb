{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "050ae04e-401d-45fe-b017-1ed1ce28c07d",
   "metadata": {},
   "source": [
    "# Virtual Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d391f8a6-c760-4ebb-abe0-e0060ccd433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "import seaborn as sns; sns.set()\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "from os.path import exists\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c6aa4081-f0bd-445e-bd70-2c5a5e88b081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4682d46c-a576-4485-aecb-a219f2bfea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/dataset.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61a3f197-2d93-4bb8-8e87-78e73661a5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Consumer_number</th>\n",
       "      <th>Consumption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1541281</th>\n",
       "      <td>1</td>\n",
       "      <td>21018</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541282</th>\n",
       "      <td>1</td>\n",
       "      <td>49120</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541283</th>\n",
       "      <td>1</td>\n",
       "      <td>17940</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541284</th>\n",
       "      <td>1</td>\n",
       "      <td>14273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541285</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Month  Consumer_number  Consumption\n",
       "1541281      1            21018           11\n",
       "1541282      1            49120            5\n",
       "1541283      1            17940            0\n",
       "1541284      1            14273            1\n",
       "1541285      1                5            8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df.drop(columns={'Installation_zone', 'Consumer_type', 'Measure_method', 'Consumo', 'Installation_number', 'Zona_cob', 'Counter_number', 'Counter_manufacturer', 'Counter_caliber'})\n",
    "df1[\"Consumer_number\"] = df1[\"Consumer_number\"].astype(int)\n",
    "df1 = df1[df1['Year'] == 2019].drop(columns={'Year'})\n",
    "df1 = df1.drop_duplicates().fillna(0)\n",
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2621e84-7203-43e6-9815-c882a7c89469",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07a693ce-5b24-4920-8359-faa28d8630ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Consumer_number</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27796</th>\n",
       "      <td>55640</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27797</th>\n",
       "      <td>55641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27798</th>\n",
       "      <td>55642</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27799</th>\n",
       "      <td>55643</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27800</th>\n",
       "      <td>2132971</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Consumer_number    1    2    3    4    5    6    7    8    9   10   11  \\\n",
       "27796            55640  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "27797            55641  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "27798            55642  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "27799            55643  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "27800          2132971  2.0  2.0  2.0  1.0  3.0  2.0  0.0  0.0  0.0  4.0  2.0   \n",
       "\n",
       "        12  \n",
       "27796  0.0  \n",
       "27797  0.0  \n",
       "27798  0.0  \n",
       "27799  0.0  \n",
       "27800  2.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vae = df1.groupby(['Consumer_number', 'Month'], as_index = False).sum()\n",
    "df_vae = df_vae.pivot(index = 'Consumer_number', columns = 'Month', values = 'Consumption')\n",
    "df_vae = df_vae.fillna(0)\n",
    "df_vae = df_vae.rename_axis(None,axis=1).set_axis(monthes, inplace=False, axis=1).reset_index()\n",
    "df_vae.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8340cf55-555e-45d4-aeb2-0e0dc7c832c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Consumer_number</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27795</th>\n",
       "      <td>55639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27796</th>\n",
       "      <td>55640</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27797</th>\n",
       "      <td>55641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27798</th>\n",
       "      <td>55642</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27800</th>\n",
       "      <td>2132971</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Consumer_number    1    2    3    4    5    6    7    8    9   10   11  \\\n",
       "27795            55639  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "27796            55640  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "27797            55641  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "27798            55642  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "27800          2132971  2.0  2.0  2.0  1.0  3.0  2.0  0.0  0.0  0.0  4.0  2.0   \n",
       "\n",
       "        12  \n",
       "27795  0.0  \n",
       "27796  0.0  \n",
       "27797  0.0  \n",
       "27798  0.0  \n",
       "27800  2.0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vae = df_vae.drop(27799)\n",
    "df_vae.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c34dffd4-ff47-460c-96f1-fe90d53435bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27800, 13)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vae.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "66708211-228b-4285-8ce9-b6ed2ebfec78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>332.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>308.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>131.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27795</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27796</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27797</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27798</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27800</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27800 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1      2      3     4      5      6      7      8      9      10  \\\n",
       "0      332.0  330.0  331.0  31.0  208.0  231.0  219.0  308.0  259.0  224.0   \n",
       "1        8.0    0.0    4.0  22.0    8.0    9.0    8.0    9.0    8.0    8.0   \n",
       "2        2.0   13.0    8.0   0.0    4.0    2.0    3.0   12.0    8.0    2.0   \n",
       "3        6.0   19.0   13.0   0.0    7.0    4.0    6.0    5.0    0.0    3.0   \n",
       "4        0.0    0.0    0.0   0.0   30.0    0.0   15.0    8.0   12.0    0.0   \n",
       "...      ...    ...    ...   ...    ...    ...    ...    ...    ...    ...   \n",
       "27795    0.0    0.0    0.0   0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "27796    0.0    0.0    0.0   0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "27797    0.0    0.0    0.0   0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "27798    0.0    0.0    0.0   0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "27800    2.0    2.0    2.0   1.0    3.0    2.0    0.0    0.0    0.0    4.0   \n",
       "\n",
       "          11     12  \n",
       "0      268.0  131.0  \n",
       "1        9.0    7.0  \n",
       "2        5.0    0.0  \n",
       "3       12.0    8.0  \n",
       "4        4.0    0.0  \n",
       "...      ...    ...  \n",
       "27795    0.0    0.0  \n",
       "27796    0.0    0.0  \n",
       "27797    0.0    0.0  \n",
       "27798    0.0    0.0  \n",
       "27800    2.0    2.0  \n",
       "\n",
       "[27800 rows x 12 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vae_final = df_vae.drop(columns={'Consumer_number'})\n",
    "df_vae_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "74763f96-3b01-4e3f-ab9b-d8bc26ec0cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.996988</td>\n",
       "      <td>0.093373</td>\n",
       "      <td>0.626506</td>\n",
       "      <td>0.695783</td>\n",
       "      <td>0.659639</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.780120</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.394578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.318182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.153846</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.315789</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.421053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27795</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27796</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27797</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27798</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27800</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27800 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             1         2         3         4         5         6         7   \\\n",
       "0      1.000000  0.993976  0.996988  0.093373  0.626506  0.695783  0.659639   \n",
       "1      0.363636  0.000000  0.181818  1.000000  0.363636  0.409091  0.363636   \n",
       "2      0.153846  1.000000  0.615385  0.000000  0.307692  0.153846  0.230769   \n",
       "3      0.315789  1.000000  0.684211  0.000000  0.368421  0.210526  0.315789   \n",
       "4      0.000000  0.000000  0.000000  0.000000  1.000000  0.000000  0.500000   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "27795       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "27796       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "27797       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "27798       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "27800  0.500000  0.500000  0.500000  0.250000  0.750000  0.500000  0.000000   \n",
       "\n",
       "             8         9         10        11        12  \n",
       "0      0.927711  0.780120  0.674699  0.807229  0.394578  \n",
       "1      0.409091  0.363636  0.363636  0.409091  0.318182  \n",
       "2      0.923077  0.615385  0.153846  0.384615  0.000000  \n",
       "3      0.263158  0.000000  0.157895  0.631579  0.421053  \n",
       "4      0.266667  0.400000  0.000000  0.133333  0.000000  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "27795       NaN       NaN       NaN       NaN       NaN  \n",
       "27796       NaN       NaN       NaN       NaN       NaN  \n",
       "27797       NaN       NaN       NaN       NaN       NaN  \n",
       "27798       NaN       NaN       NaN       NaN       NaN  \n",
       "27800  0.000000  0.000000  1.000000  0.500000  0.500000  \n",
       "\n",
       "[27800 rows x 12 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.preprocessing import Normalizer\n",
    "df_vae = df_vae_final.apply(lambda x: x/x.max(), axis=1)\n",
    "df_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a69669e3-8d0e-4f2d-897b-4377b1024fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.996988</td>\n",
       "      <td>0.093373</td>\n",
       "      <td>0.626506</td>\n",
       "      <td>0.695783</td>\n",
       "      <td>0.659639</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.780120</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.394578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.318182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.153846</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.315789</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.421053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27795</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27796</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27797</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27798</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27800</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27800 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             1         2         3         4         5         6         7   \\\n",
       "0      1.000000  0.993976  0.996988  0.093373  0.626506  0.695783  0.659639   \n",
       "1      0.363636  0.000000  0.181818  1.000000  0.363636  0.409091  0.363636   \n",
       "2      0.153846  1.000000  0.615385  0.000000  0.307692  0.153846  0.230769   \n",
       "3      0.315789  1.000000  0.684211  0.000000  0.368421  0.210526  0.315789   \n",
       "4      0.000000  0.000000  0.000000  0.000000  1.000000  0.000000  0.500000   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "27795  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "27796  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "27797  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "27798  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "27800  0.500000  0.500000  0.500000  0.250000  0.750000  0.500000  0.000000   \n",
       "\n",
       "             8         9         10        11        12  \n",
       "0      0.927711  0.780120  0.674699  0.807229  0.394578  \n",
       "1      0.409091  0.363636  0.363636  0.409091  0.318182  \n",
       "2      0.923077  0.615385  0.153846  0.384615  0.000000  \n",
       "3      0.263158  0.000000  0.157895  0.631579  0.421053  \n",
       "4      0.266667  0.400000  0.000000  0.133333  0.000000  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "27795  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "27796  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "27797  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "27798  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "27800  0.000000  0.000000  1.000000  0.500000  0.500000  \n",
       "\n",
       "[27800 rows x 12 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vae = df_vae.fillna(0)\n",
    "df_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9567179a-e4c2-48ec-a408-36c1f3df0280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.031912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.153358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.425591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.916678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.690488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333595</th>\n",
       "      <td>-0.120947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333596</th>\n",
       "      <td>-0.124068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333597</th>\n",
       "      <td>-0.044599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333598</th>\n",
       "      <td>-0.062927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333599</th>\n",
       "      <td>-0.066218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>333600 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "0       10.031912\n",
       "1       10.153358\n",
       "2       10.425591\n",
       "3        0.916678\n",
       "4        6.690488\n",
       "...           ...\n",
       "333595  -0.120947\n",
       "333596  -0.124068\n",
       "333597  -0.044599\n",
       "333598  -0.062927\n",
       "333599  -0.066218\n",
       "\n",
       "[333600 rows x 1 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vae_chunks = np.array_split(df_vae_final,20)\n",
    "# vae_chunks[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b1b9314b-916a-4b2f-a971-f820da8f392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "  def __init__(self, df):\n",
    "\n",
    "    x= df.iloc[:,0:12].values\n",
    "    # y= df.iloc[:,1].values\n",
    "\n",
    "    self.x_train=torch.tensor(x, dtype=torch.float32)\n",
    "    # self.y_train=torch.tensor(y,dtype=torch.float32)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.x_train)\n",
    "  \n",
    "  def __getitem__(self,idx):\n",
    "    return self.x_train[idx], torch.tensor(0)\n",
    "        # self.y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "be2a0894-3f85-4318-bc80-b75febd0b38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.0000, 0.9940, 0.9970, 0.0934, 0.6265, 0.6958, 0.6596, 0.9277, 0.7801,\n",
       "         0.6747, 0.8072, 0.3946]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDs=MyDataset(df_vae)\n",
    "myDs.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4cccb0-720c-49b2-bb19-1859831dc26d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1c79bcbe-b8ae-41f8-b84f-12f5e0aba308",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# DataLoader is used to load the dataset\n",
    "# for training\n",
    "loader = torch.utils.data.DataLoader(dataset = myDs, batch_size = batch_size,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e180305a-a279-4c96-8ff7-6a76ffd99f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.9444, 0.5556,  ..., 0.8333, 0.5000, 0.6667],\n",
       "         ...,\n",
       "         [0.4286, 0.4762, 0.4762,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 0.8750, 0.7500, 0.7500],\n",
       "         [1.0000, 0.5000, 0.6250,  ..., 0.6250, 0.6250, 0.6250]]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smth = next(iter(loader))\n",
    "smth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6e7d2440-e941-4271-ae76-5a7c6cfd75fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(12, 8)\n",
    "        self.fc21 = torch.nn.Linear(8, 2)\n",
    "        self.fc22 = torch.nn.Linear(8, 2)\n",
    "        self.fc3 = torch.nn.Linear(2, 8)\n",
    "        self.fc4 = torch.nn.Linear(8, 12)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if torch.cuda.is_available():\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return F.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d6dd888e-ddeb-4074-94a1-f877f5c5ef1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\.conda\\env\\pandia\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "# Model Initialization\n",
    "model = VAE()\n",
    "#moving to gpu\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    \n",
    "reconstruction_function = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    recon_x: generating images\n",
    "    x: origin images\n",
    "    mu: latent mean\n",
    "    logvar: latent log variance\n",
    "    \"\"\"\n",
    "    BCE = reconstruction_function(recon_x, x)  # mse loss\n",
    "    # print(\"BCE:\", BCE)\n",
    "    # loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    # print(\"KLD_element:\", KLD_element)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "    # print(\"KLD:\", KLD)\n",
    "    # KL divergence\n",
    "    # print(\"BCE + KLD:\", BCE + KLD)\n",
    "    return BCE + KLD\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "fe00f711-3e2e-4ea5-a3f4-3c41f691fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"vae.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7347a17d-0acd-415d-a64d-43d0ac1bd6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/27800 (0%)]\tLoss: 1.471144\n",
      "Train Epoch: 0 [12800/27800 (46%)]\tLoss: 1.523675\n",
      "Train Epoch: 0 [25600/27800 (92%)]\tLoss: 1.590726\n",
      "====> Epoch: 0 Average loss: 1.5292\n",
      "Train Epoch: 1 [0/27800 (0%)]\tLoss: 1.608548\n",
      "Train Epoch: 1 [12800/27800 (46%)]\tLoss: 1.562280\n",
      "Train Epoch: 1 [25600/27800 (92%)]\tLoss: 1.602880\n",
      "====> Epoch: 1 Average loss: 1.5325\n",
      "Train Epoch: 2 [0/27800 (0%)]\tLoss: 1.525115\n",
      "Train Epoch: 2 [12800/27800 (46%)]\tLoss: 1.519312\n",
      "Train Epoch: 2 [25600/27800 (92%)]\tLoss: 1.550952\n",
      "====> Epoch: 2 Average loss: 1.5383\n",
      "Train Epoch: 3 [0/27800 (0%)]\tLoss: 1.535538\n",
      "Train Epoch: 3 [12800/27800 (46%)]\tLoss: 1.400029\n",
      "Train Epoch: 3 [25600/27800 (92%)]\tLoss: 1.440854\n",
      "====> Epoch: 3 Average loss: 1.5366\n",
      "Train Epoch: 4 [0/27800 (0%)]\tLoss: 1.591942\n",
      "Train Epoch: 4 [12800/27800 (46%)]\tLoss: 1.533452\n",
      "Train Epoch: 4 [25600/27800 (92%)]\tLoss: 1.518484\n",
      "====> Epoch: 4 Average loss: 1.5375\n",
      "Train Epoch: 5 [0/27800 (0%)]\tLoss: 1.522334\n",
      "Train Epoch: 5 [12800/27800 (46%)]\tLoss: 1.462837\n",
      "Train Epoch: 5 [25600/27800 (92%)]\tLoss: 1.409034\n",
      "====> Epoch: 5 Average loss: 1.5348\n",
      "Train Epoch: 6 [0/27800 (0%)]\tLoss: 1.587142\n",
      "Train Epoch: 6 [12800/27800 (46%)]\tLoss: 1.517157\n",
      "Train Epoch: 6 [25600/27800 (92%)]\tLoss: 1.559071\n",
      "====> Epoch: 6 Average loss: 1.5421\n",
      "Train Epoch: 7 [0/27800 (0%)]\tLoss: 1.542549\n",
      "Train Epoch: 7 [12800/27800 (46%)]\tLoss: 1.545904\n",
      "Train Epoch: 7 [25600/27800 (92%)]\tLoss: 1.577445\n",
      "====> Epoch: 7 Average loss: 1.5333\n",
      "Train Epoch: 8 [0/27800 (0%)]\tLoss: 1.526049\n",
      "Train Epoch: 8 [12800/27800 (46%)]\tLoss: 1.562479\n",
      "Train Epoch: 8 [25600/27800 (92%)]\tLoss: 1.601563\n",
      "====> Epoch: 8 Average loss: 1.5348\n",
      "Train Epoch: 9 [0/27800 (0%)]\tLoss: 1.503631\n",
      "Train Epoch: 9 [12800/27800 (46%)]\tLoss: 1.552686\n",
      "Train Epoch: 9 [25600/27800 (92%)]\tLoss: 1.463443\n",
      "====> Epoch: 9 Average loss: 1.5344\n",
      "Train Epoch: 10 [0/27800 (0%)]\tLoss: 1.576395\n",
      "Train Epoch: 10 [12800/27800 (46%)]\tLoss: 1.551053\n",
      "Train Epoch: 10 [25600/27800 (92%)]\tLoss: 1.480668\n",
      "====> Epoch: 10 Average loss: 1.5281\n",
      "Train Epoch: 11 [0/27800 (0%)]\tLoss: 1.596465\n",
      "Train Epoch: 11 [12800/27800 (46%)]\tLoss: 1.642843\n",
      "Train Epoch: 11 [25600/27800 (92%)]\tLoss: 1.570333\n",
      "====> Epoch: 11 Average loss: 1.5364\n",
      "Train Epoch: 12 [0/27800 (0%)]\tLoss: 1.664619\n",
      "Train Epoch: 12 [12800/27800 (46%)]\tLoss: 1.667880\n",
      "Train Epoch: 12 [25600/27800 (92%)]\tLoss: 1.467538\n",
      "====> Epoch: 12 Average loss: 1.5359\n",
      "Train Epoch: 13 [0/27800 (0%)]\tLoss: 1.667982\n",
      "Train Epoch: 13 [12800/27800 (46%)]\tLoss: 1.583153\n",
      "Train Epoch: 13 [25600/27800 (92%)]\tLoss: 1.507428\n",
      "====> Epoch: 13 Average loss: 1.5344\n",
      "Train Epoch: 14 [0/27800 (0%)]\tLoss: 1.473883\n",
      "Train Epoch: 14 [12800/27800 (46%)]\tLoss: 1.467214\n",
      "Train Epoch: 14 [25600/27800 (92%)]\tLoss: 1.489308\n",
      "====> Epoch: 14 Average loss: 1.5331\n",
      "Train Epoch: 15 [0/27800 (0%)]\tLoss: 1.529734\n",
      "Train Epoch: 15 [12800/27800 (46%)]\tLoss: 1.576066\n",
      "Train Epoch: 15 [25600/27800 (92%)]\tLoss: 1.610085\n",
      "====> Epoch: 15 Average loss: 1.5395\n",
      "Train Epoch: 16 [0/27800 (0%)]\tLoss: 1.513962\n",
      "Train Epoch: 16 [12800/27800 (46%)]\tLoss: 1.520749\n",
      "Train Epoch: 16 [25600/27800 (92%)]\tLoss: 1.506784\n",
      "====> Epoch: 16 Average loss: 1.5337\n",
      "Train Epoch: 17 [0/27800 (0%)]\tLoss: 1.469092\n",
      "Train Epoch: 17 [12800/27800 (46%)]\tLoss: 1.532698\n",
      "Train Epoch: 17 [25600/27800 (92%)]\tLoss: 1.490711\n",
      "====> Epoch: 17 Average loss: 1.5330\n",
      "Train Epoch: 18 [0/27800 (0%)]\tLoss: 1.449798\n",
      "Train Epoch: 18 [12800/27800 (46%)]\tLoss: 1.507529\n",
      "Train Epoch: 18 [25600/27800 (92%)]\tLoss: 1.534082\n",
      "====> Epoch: 18 Average loss: 1.5388\n",
      "Train Epoch: 19 [0/27800 (0%)]\tLoss: 1.505932\n",
      "Train Epoch: 19 [12800/27800 (46%)]\tLoss: 1.521370\n",
      "Train Epoch: 19 [25600/27800 (92%)]\tLoss: 1.456106\n",
      "====> Epoch: 19 Average loss: 1.5325\n",
      "Train Epoch: 20 [0/27800 (0%)]\tLoss: 1.510537\n",
      "Train Epoch: 20 [12800/27800 (46%)]\tLoss: 1.648490\n",
      "Train Epoch: 20 [25600/27800 (92%)]\tLoss: 1.538381\n",
      "====> Epoch: 20 Average loss: 1.5347\n",
      "Train Epoch: 21 [0/27800 (0%)]\tLoss: 1.505149\n",
      "Train Epoch: 21 [12800/27800 (46%)]\tLoss: 1.583785\n",
      "Train Epoch: 21 [25600/27800 (92%)]\tLoss: 1.536749\n",
      "====> Epoch: 21 Average loss: 1.5318\n",
      "Train Epoch: 22 [0/27800 (0%)]\tLoss: 1.572406\n",
      "Train Epoch: 22 [12800/27800 (46%)]\tLoss: 1.524703\n",
      "Train Epoch: 22 [25600/27800 (92%)]\tLoss: 1.581512\n",
      "====> Epoch: 22 Average loss: 1.5366\n",
      "Train Epoch: 23 [0/27800 (0%)]\tLoss: 1.549719\n",
      "Train Epoch: 23 [12800/27800 (46%)]\tLoss: 1.511957\n",
      "Train Epoch: 23 [25600/27800 (92%)]\tLoss: 1.523330\n",
      "====> Epoch: 23 Average loss: 1.5362\n",
      "Train Epoch: 24 [0/27800 (0%)]\tLoss: 1.517011\n",
      "Train Epoch: 24 [12800/27800 (46%)]\tLoss: 1.556424\n",
      "Train Epoch: 24 [25600/27800 (92%)]\tLoss: 1.453290\n",
      "====> Epoch: 24 Average loss: 1.5399\n",
      "Train Epoch: 25 [0/27800 (0%)]\tLoss: 1.562101\n",
      "Train Epoch: 25 [12800/27800 (46%)]\tLoss: 1.555222\n",
      "Train Epoch: 25 [25600/27800 (92%)]\tLoss: 1.445067\n",
      "====> Epoch: 25 Average loss: 1.5314\n",
      "Train Epoch: 26 [0/27800 (0%)]\tLoss: 1.423913\n",
      "Train Epoch: 26 [12800/27800 (46%)]\tLoss: 1.436031\n",
      "Train Epoch: 26 [25600/27800 (92%)]\tLoss: 1.507314\n",
      "====> Epoch: 26 Average loss: 1.5324\n",
      "Train Epoch: 27 [0/27800 (0%)]\tLoss: 1.479798\n",
      "Train Epoch: 27 [12800/27800 (46%)]\tLoss: 1.555382\n",
      "Train Epoch: 27 [25600/27800 (92%)]\tLoss: 1.516841\n",
      "====> Epoch: 27 Average loss: 1.5340\n",
      "Train Epoch: 28 [0/27800 (0%)]\tLoss: 1.450955\n",
      "Train Epoch: 28 [12800/27800 (46%)]\tLoss: 1.508047\n",
      "Train Epoch: 28 [25600/27800 (92%)]\tLoss: 1.560881\n",
      "====> Epoch: 28 Average loss: 1.5311\n",
      "Train Epoch: 29 [0/27800 (0%)]\tLoss: 1.575654\n",
      "Train Epoch: 29 [12800/27800 (46%)]\tLoss: 1.485192\n",
      "Train Epoch: 29 [25600/27800 (92%)]\tLoss: 1.511711\n",
      "====> Epoch: 29 Average loss: 1.5410\n",
      "Train Epoch: 30 [0/27800 (0%)]\tLoss: 1.483273\n",
      "Train Epoch: 30 [12800/27800 (46%)]\tLoss: 1.484697\n",
      "Train Epoch: 30 [25600/27800 (92%)]\tLoss: 1.560873\n",
      "====> Epoch: 30 Average loss: 1.5353\n",
      "Train Epoch: 31 [0/27800 (0%)]\tLoss: 1.458338\n",
      "Train Epoch: 31 [12800/27800 (46%)]\tLoss: 1.437940\n",
      "Train Epoch: 31 [25600/27800 (92%)]\tLoss: 1.501993\n",
      "====> Epoch: 31 Average loss: 1.5325\n",
      "Train Epoch: 32 [0/27800 (0%)]\tLoss: 1.471788\n",
      "Train Epoch: 32 [12800/27800 (46%)]\tLoss: 1.443610\n",
      "Train Epoch: 32 [25600/27800 (92%)]\tLoss: 1.516271\n",
      "====> Epoch: 32 Average loss: 1.5314\n",
      "Train Epoch: 33 [0/27800 (0%)]\tLoss: 1.518338\n",
      "Train Epoch: 33 [12800/27800 (46%)]\tLoss: 1.555330\n",
      "Train Epoch: 33 [25600/27800 (92%)]\tLoss: 1.418901\n",
      "====> Epoch: 33 Average loss: 1.5386\n",
      "Train Epoch: 34 [0/27800 (0%)]\tLoss: 1.466483\n",
      "Train Epoch: 34 [12800/27800 (46%)]\tLoss: 1.576137\n",
      "Train Epoch: 34 [25600/27800 (92%)]\tLoss: 1.574647\n",
      "====> Epoch: 34 Average loss: 1.5326\n",
      "Train Epoch: 35 [0/27800 (0%)]\tLoss: 1.542896\n",
      "Train Epoch: 35 [12800/27800 (46%)]\tLoss: 1.412373\n",
      "Train Epoch: 35 [25600/27800 (92%)]\tLoss: 1.570809\n",
      "====> Epoch: 35 Average loss: 1.5379\n",
      "Train Epoch: 36 [0/27800 (0%)]\tLoss: 1.544698\n",
      "Train Epoch: 36 [12800/27800 (46%)]\tLoss: 1.676904\n",
      "Train Epoch: 36 [25600/27800 (92%)]\tLoss: 1.448061\n",
      "====> Epoch: 36 Average loss: 1.5362\n",
      "Train Epoch: 37 [0/27800 (0%)]\tLoss: 1.663974\n",
      "Train Epoch: 37 [12800/27800 (46%)]\tLoss: 1.433598\n",
      "Train Epoch: 37 [25600/27800 (92%)]\tLoss: 1.602261\n",
      "====> Epoch: 37 Average loss: 1.5358\n",
      "Train Epoch: 38 [0/27800 (0%)]\tLoss: 1.485648\n",
      "Train Epoch: 38 [12800/27800 (46%)]\tLoss: 1.426050\n",
      "Train Epoch: 38 [25600/27800 (92%)]\tLoss: 1.479597\n",
      "====> Epoch: 38 Average loss: 1.5342\n",
      "Train Epoch: 39 [0/27800 (0%)]\tLoss: 1.569591\n",
      "Train Epoch: 39 [12800/27800 (46%)]\tLoss: 1.463512\n",
      "Train Epoch: 39 [25600/27800 (92%)]\tLoss: 1.384557\n",
      "====> Epoch: 39 Average loss: 1.5336\n",
      "Train Epoch: 40 [0/27800 (0%)]\tLoss: 1.640799\n",
      "Train Epoch: 40 [12800/27800 (46%)]\tLoss: 1.409588\n",
      "Train Epoch: 40 [25600/27800 (92%)]\tLoss: 1.560982\n",
      "====> Epoch: 40 Average loss: 1.5403\n",
      "Train Epoch: 41 [0/27800 (0%)]\tLoss: 1.516078\n",
      "Train Epoch: 41 [12800/27800 (46%)]\tLoss: 1.521589\n",
      "Train Epoch: 41 [25600/27800 (92%)]\tLoss: 1.468936\n",
      "====> Epoch: 41 Average loss: 1.5361\n",
      "Train Epoch: 42 [0/27800 (0%)]\tLoss: 1.607127\n",
      "Train Epoch: 42 [12800/27800 (46%)]\tLoss: 1.619324\n",
      "Train Epoch: 42 [25600/27800 (92%)]\tLoss: 1.517582\n",
      "====> Epoch: 42 Average loss: 1.5369\n",
      "Train Epoch: 43 [0/27800 (0%)]\tLoss: 1.508077\n",
      "Train Epoch: 43 [12800/27800 (46%)]\tLoss: 1.471636\n",
      "Train Epoch: 43 [25600/27800 (92%)]\tLoss: 1.584230\n",
      "====> Epoch: 43 Average loss: 1.5362\n",
      "Train Epoch: 44 [0/27800 (0%)]\tLoss: 1.496472\n",
      "Train Epoch: 44 [12800/27800 (46%)]\tLoss: 1.463763\n",
      "Train Epoch: 44 [25600/27800 (92%)]\tLoss: 1.488965\n",
      "====> Epoch: 44 Average loss: 1.5308\n",
      "Train Epoch: 45 [0/27800 (0%)]\tLoss: 1.560491\n",
      "Train Epoch: 45 [12800/27800 (46%)]\tLoss: 1.591191\n",
      "Train Epoch: 45 [25600/27800 (92%)]\tLoss: 1.516862\n",
      "====> Epoch: 45 Average loss: 1.5418\n",
      "Train Epoch: 46 [0/27800 (0%)]\tLoss: 1.651767\n",
      "Train Epoch: 46 [12800/27800 (46%)]\tLoss: 1.568806\n",
      "Train Epoch: 46 [25600/27800 (92%)]\tLoss: 1.591542\n",
      "====> Epoch: 46 Average loss: 1.5305\n",
      "Train Epoch: 47 [0/27800 (0%)]\tLoss: 1.568137\n",
      "Train Epoch: 47 [12800/27800 (46%)]\tLoss: 1.561960\n",
      "Train Epoch: 47 [25600/27800 (92%)]\tLoss: 1.542832\n",
      "====> Epoch: 47 Average loss: 1.5368\n",
      "Train Epoch: 48 [0/27800 (0%)]\tLoss: 1.664780\n",
      "Train Epoch: 48 [12800/27800 (46%)]\tLoss: 1.600143\n",
      "Train Epoch: 48 [25600/27800 (92%)]\tLoss: 1.594567\n",
      "====> Epoch: 48 Average loss: 1.5406\n",
      "Train Epoch: 49 [0/27800 (0%)]\tLoss: 1.584965\n",
      "Train Epoch: 49 [12800/27800 (46%)]\tLoss: 1.473488\n",
      "Train Epoch: 49 [25600/27800 (92%)]\tLoss: 1.352818\n",
      "====> Epoch: 49 Average loss: 1.5313\n",
      "Train Epoch: 50 [0/27800 (0%)]\tLoss: 1.518481\n",
      "Train Epoch: 50 [12800/27800 (46%)]\tLoss: 1.660039\n",
      "Train Epoch: 50 [25600/27800 (92%)]\tLoss: 1.389447\n",
      "====> Epoch: 50 Average loss: 1.5323\n",
      "Train Epoch: 51 [0/27800 (0%)]\tLoss: 1.545258\n",
      "Train Epoch: 51 [12800/27800 (46%)]\tLoss: 1.470123\n",
      "Train Epoch: 51 [25600/27800 (92%)]\tLoss: 1.627150\n",
      "====> Epoch: 51 Average loss: 1.5347\n",
      "Train Epoch: 52 [0/27800 (0%)]\tLoss: 1.526042\n",
      "Train Epoch: 52 [12800/27800 (46%)]\tLoss: 1.385950\n",
      "Train Epoch: 52 [25600/27800 (92%)]\tLoss: 1.556987\n",
      "====> Epoch: 52 Average loss: 1.5318\n",
      "Train Epoch: 53 [0/27800 (0%)]\tLoss: 1.579987\n",
      "Train Epoch: 53 [12800/27800 (46%)]\tLoss: 1.631672\n",
      "Train Epoch: 53 [25600/27800 (92%)]\tLoss: 1.578869\n",
      "====> Epoch: 53 Average loss: 1.5347\n",
      "Train Epoch: 54 [0/27800 (0%)]\tLoss: 1.555881\n",
      "Train Epoch: 54 [12800/27800 (46%)]\tLoss: 1.580526\n",
      "Train Epoch: 54 [25600/27800 (92%)]\tLoss: 1.609578\n",
      "====> Epoch: 54 Average loss: 1.5310\n",
      "Train Epoch: 55 [0/27800 (0%)]\tLoss: 1.417549\n",
      "Train Epoch: 55 [12800/27800 (46%)]\tLoss: 1.538707\n",
      "Train Epoch: 55 [25600/27800 (92%)]\tLoss: 1.549365\n",
      "====> Epoch: 55 Average loss: 1.5443\n",
      "Train Epoch: 56 [0/27800 (0%)]\tLoss: 1.522069\n",
      "Train Epoch: 56 [12800/27800 (46%)]\tLoss: 1.531717\n",
      "Train Epoch: 56 [25600/27800 (92%)]\tLoss: 1.553807\n",
      "====> Epoch: 56 Average loss: 1.5364\n",
      "Train Epoch: 57 [0/27800 (0%)]\tLoss: 1.512180\n",
      "Train Epoch: 57 [12800/27800 (46%)]\tLoss: 1.452772\n",
      "Train Epoch: 57 [25600/27800 (92%)]\tLoss: 1.519027\n",
      "====> Epoch: 57 Average loss: 1.5351\n",
      "Train Epoch: 58 [0/27800 (0%)]\tLoss: 1.530898\n",
      "Train Epoch: 58 [12800/27800 (46%)]\tLoss: 1.568862\n",
      "Train Epoch: 58 [25600/27800 (92%)]\tLoss: 1.518662\n",
      "====> Epoch: 58 Average loss: 1.5304\n",
      "Train Epoch: 59 [0/27800 (0%)]\tLoss: 1.571375\n",
      "Train Epoch: 59 [12800/27800 (46%)]\tLoss: 1.369927\n",
      "Train Epoch: 59 [25600/27800 (92%)]\tLoss: 1.504633\n",
      "====> Epoch: 59 Average loss: 1.5357\n",
      "Train Epoch: 60 [0/27800 (0%)]\tLoss: 1.475415\n",
      "Train Epoch: 60 [12800/27800 (46%)]\tLoss: 1.652230\n",
      "Train Epoch: 60 [25600/27800 (92%)]\tLoss: 1.476864\n",
      "====> Epoch: 60 Average loss: 1.5275\n",
      "Train Epoch: 61 [0/27800 (0%)]\tLoss: 1.487921\n",
      "Train Epoch: 61 [12800/27800 (46%)]\tLoss: 1.562553\n",
      "Train Epoch: 61 [25600/27800 (92%)]\tLoss: 1.567200\n",
      "====> Epoch: 61 Average loss: 1.5403\n",
      "Train Epoch: 62 [0/27800 (0%)]\tLoss: 1.524549\n",
      "Train Epoch: 62 [12800/27800 (46%)]\tLoss: 1.507102\n",
      "Train Epoch: 62 [25600/27800 (92%)]\tLoss: 1.504539\n",
      "====> Epoch: 62 Average loss: 1.5359\n",
      "Train Epoch: 63 [0/27800 (0%)]\tLoss: 1.492973\n",
      "Train Epoch: 63 [12800/27800 (46%)]\tLoss: 1.489599\n",
      "Train Epoch: 63 [25600/27800 (92%)]\tLoss: 1.481106\n",
      "====> Epoch: 63 Average loss: 1.5381\n",
      "Train Epoch: 64 [0/27800 (0%)]\tLoss: 1.569855\n",
      "Train Epoch: 64 [12800/27800 (46%)]\tLoss: 1.436091\n",
      "Train Epoch: 64 [25600/27800 (92%)]\tLoss: 1.502331\n",
      "====> Epoch: 64 Average loss: 1.5350\n",
      "Train Epoch: 65 [0/27800 (0%)]\tLoss: 1.509991\n",
      "Train Epoch: 65 [12800/27800 (46%)]\tLoss: 1.517194\n",
      "Train Epoch: 65 [25600/27800 (92%)]\tLoss: 1.566448\n",
      "====> Epoch: 65 Average loss: 1.5351\n",
      "Train Epoch: 66 [0/27800 (0%)]\tLoss: 1.494301\n",
      "Train Epoch: 66 [12800/27800 (46%)]\tLoss: 1.426105\n",
      "Train Epoch: 66 [25600/27800 (92%)]\tLoss: 1.546669\n",
      "====> Epoch: 66 Average loss: 1.5397\n",
      "Train Epoch: 67 [0/27800 (0%)]\tLoss: 1.527845\n",
      "Train Epoch: 67 [12800/27800 (46%)]\tLoss: 1.556300\n",
      "Train Epoch: 67 [25600/27800 (92%)]\tLoss: 1.488374\n",
      "====> Epoch: 67 Average loss: 1.5375\n",
      "Train Epoch: 68 [0/27800 (0%)]\tLoss: 1.500022\n",
      "Train Epoch: 68 [12800/27800 (46%)]\tLoss: 1.601730\n",
      "Train Epoch: 68 [25600/27800 (92%)]\tLoss: 1.456341\n",
      "====> Epoch: 68 Average loss: 1.5351\n",
      "Train Epoch: 69 [0/27800 (0%)]\tLoss: 1.446979\n",
      "Train Epoch: 69 [12800/27800 (46%)]\tLoss: 1.516783\n",
      "Train Epoch: 69 [25600/27800 (92%)]\tLoss: 1.533826\n",
      "====> Epoch: 69 Average loss: 1.5304\n",
      "Train Epoch: 70 [0/27800 (0%)]\tLoss: 1.516997\n",
      "Train Epoch: 70 [12800/27800 (46%)]\tLoss: 1.644269\n",
      "Train Epoch: 70 [25600/27800 (92%)]\tLoss: 1.492300\n",
      "====> Epoch: 70 Average loss: 1.5405\n",
      "Train Epoch: 71 [0/27800 (0%)]\tLoss: 1.553793\n",
      "Train Epoch: 71 [12800/27800 (46%)]\tLoss: 1.474126\n",
      "Train Epoch: 71 [25600/27800 (92%)]\tLoss: 1.581749\n",
      "====> Epoch: 71 Average loss: 1.5380\n",
      "Train Epoch: 72 [0/27800 (0%)]\tLoss: 1.495944\n",
      "Train Epoch: 72 [12800/27800 (46%)]\tLoss: 1.508389\n",
      "Train Epoch: 72 [25600/27800 (92%)]\tLoss: 1.537065\n",
      "====> Epoch: 72 Average loss: 1.5396\n",
      "Train Epoch: 73 [0/27800 (0%)]\tLoss: 1.494210\n",
      "Train Epoch: 73 [12800/27800 (46%)]\tLoss: 1.478542\n",
      "Train Epoch: 73 [25600/27800 (92%)]\tLoss: 1.580859\n",
      "====> Epoch: 73 Average loss: 1.5382\n",
      "Train Epoch: 74 [0/27800 (0%)]\tLoss: 1.601321\n",
      "Train Epoch: 74 [12800/27800 (46%)]\tLoss: 1.518985\n",
      "Train Epoch: 74 [25600/27800 (92%)]\tLoss: 1.465803\n",
      "====> Epoch: 74 Average loss: 1.5386\n",
      "Train Epoch: 75 [0/27800 (0%)]\tLoss: 1.416171\n",
      "Train Epoch: 75 [12800/27800 (46%)]\tLoss: 1.567232\n",
      "Train Epoch: 75 [25600/27800 (92%)]\tLoss: 1.548110\n",
      "====> Epoch: 75 Average loss: 1.5315\n",
      "Train Epoch: 76 [0/27800 (0%)]\tLoss: 1.557288\n",
      "Train Epoch: 76 [12800/27800 (46%)]\tLoss: 1.624978\n",
      "Train Epoch: 76 [25600/27800 (92%)]\tLoss: 1.528491\n",
      "====> Epoch: 76 Average loss: 1.5324\n",
      "Train Epoch: 77 [0/27800 (0%)]\tLoss: 1.512643\n",
      "Train Epoch: 77 [12800/27800 (46%)]\tLoss: 1.532977\n",
      "Train Epoch: 77 [25600/27800 (92%)]\tLoss: 1.568597\n",
      "====> Epoch: 77 Average loss: 1.5361\n",
      "Train Epoch: 78 [0/27800 (0%)]\tLoss: 1.482835\n",
      "Train Epoch: 78 [12800/27800 (46%)]\tLoss: 1.567346\n",
      "Train Epoch: 78 [25600/27800 (92%)]\tLoss: 1.597153\n",
      "====> Epoch: 78 Average loss: 1.5392\n",
      "Train Epoch: 79 [0/27800 (0%)]\tLoss: 1.525930\n",
      "Train Epoch: 79 [12800/27800 (46%)]\tLoss: 1.533457\n",
      "Train Epoch: 79 [25600/27800 (92%)]\tLoss: 1.559680\n",
      "====> Epoch: 79 Average loss: 1.5376\n",
      "Train Epoch: 80 [0/27800 (0%)]\tLoss: 1.597666\n",
      "Train Epoch: 80 [12800/27800 (46%)]\tLoss: 1.640141\n",
      "Train Epoch: 80 [25600/27800 (92%)]\tLoss: 1.559054\n",
      "====> Epoch: 80 Average loss: 1.5320\n",
      "Train Epoch: 81 [0/27800 (0%)]\tLoss: 1.534081\n",
      "Train Epoch: 81 [12800/27800 (46%)]\tLoss: 1.553457\n",
      "Train Epoch: 81 [25600/27800 (92%)]\tLoss: 1.608583\n",
      "====> Epoch: 81 Average loss: 1.5408\n",
      "Train Epoch: 82 [0/27800 (0%)]\tLoss: 1.532984\n",
      "Train Epoch: 82 [12800/27800 (46%)]\tLoss: 1.499695\n",
      "Train Epoch: 82 [25600/27800 (92%)]\tLoss: 1.497878\n",
      "====> Epoch: 82 Average loss: 1.5392\n",
      "Train Epoch: 83 [0/27800 (0%)]\tLoss: 1.516842\n",
      "Train Epoch: 83 [12800/27800 (46%)]\tLoss: 1.503599\n",
      "Train Epoch: 83 [25600/27800 (92%)]\tLoss: 1.474830\n",
      "====> Epoch: 83 Average loss: 1.5423\n",
      "Train Epoch: 84 [0/27800 (0%)]\tLoss: 1.582664\n",
      "Train Epoch: 84 [12800/27800 (46%)]\tLoss: 1.471826\n",
      "Train Epoch: 84 [25600/27800 (92%)]\tLoss: 1.552296\n",
      "====> Epoch: 84 Average loss: 1.5314\n",
      "Train Epoch: 85 [0/27800 (0%)]\tLoss: 1.579454\n",
      "Train Epoch: 85 [12800/27800 (46%)]\tLoss: 1.593337\n",
      "Train Epoch: 85 [25600/27800 (92%)]\tLoss: 1.616521\n",
      "====> Epoch: 85 Average loss: 1.5400\n",
      "Train Epoch: 86 [0/27800 (0%)]\tLoss: 1.635229\n",
      "Train Epoch: 86 [12800/27800 (46%)]\tLoss: 1.473031\n",
      "Train Epoch: 86 [25600/27800 (92%)]\tLoss: 1.503085\n",
      "====> Epoch: 86 Average loss: 1.5370\n",
      "Train Epoch: 87 [0/27800 (0%)]\tLoss: 1.496894\n",
      "Train Epoch: 87 [12800/27800 (46%)]\tLoss: 1.539738\n",
      "Train Epoch: 87 [25600/27800 (92%)]\tLoss: 1.568511\n",
      "====> Epoch: 87 Average loss: 1.5328\n",
      "Train Epoch: 88 [0/27800 (0%)]\tLoss: 1.659795\n",
      "Train Epoch: 88 [12800/27800 (46%)]\tLoss: 1.496776\n",
      "Train Epoch: 88 [25600/27800 (92%)]\tLoss: 1.636725\n",
      "====> Epoch: 88 Average loss: 1.5354\n",
      "Train Epoch: 89 [0/27800 (0%)]\tLoss: 1.535871\n",
      "Train Epoch: 89 [12800/27800 (46%)]\tLoss: 1.437907\n",
      "Train Epoch: 89 [25600/27800 (92%)]\tLoss: 1.551191\n",
      "====> Epoch: 89 Average loss: 1.5331\n",
      "Train Epoch: 90 [0/27800 (0%)]\tLoss: 1.663103\n",
      "Train Epoch: 90 [12800/27800 (46%)]\tLoss: 1.543834\n",
      "Train Epoch: 90 [25600/27800 (92%)]\tLoss: 1.546372\n",
      "====> Epoch: 90 Average loss: 1.5416\n",
      "Train Epoch: 91 [0/27800 (0%)]\tLoss: 1.496808\n",
      "Train Epoch: 91 [12800/27800 (46%)]\tLoss: 1.501151\n",
      "Train Epoch: 91 [25600/27800 (92%)]\tLoss: 1.606113\n",
      "====> Epoch: 91 Average loss: 1.5403\n",
      "Train Epoch: 92 [0/27800 (0%)]\tLoss: 1.551929\n",
      "Train Epoch: 92 [12800/27800 (46%)]\tLoss: 1.596977\n",
      "Train Epoch: 92 [25600/27800 (92%)]\tLoss: 1.601300\n",
      "====> Epoch: 92 Average loss: 1.5412\n",
      "Train Epoch: 93 [0/27800 (0%)]\tLoss: 1.500358\n",
      "Train Epoch: 93 [12800/27800 (46%)]\tLoss: 1.659861\n",
      "Train Epoch: 93 [25600/27800 (92%)]\tLoss: 1.602345\n",
      "====> Epoch: 93 Average loss: 1.5350\n",
      "Train Epoch: 94 [0/27800 (0%)]\tLoss: 1.502271\n",
      "Train Epoch: 94 [12800/27800 (46%)]\tLoss: 1.486027\n",
      "Train Epoch: 94 [25600/27800 (92%)]\tLoss: 1.514734\n",
      "====> Epoch: 94 Average loss: 1.5398\n",
      "Train Epoch: 95 [0/27800 (0%)]\tLoss: 1.584475\n",
      "Train Epoch: 95 [12800/27800 (46%)]\tLoss: 1.608266\n",
      "Train Epoch: 95 [25600/27800 (92%)]\tLoss: 1.625698\n",
      "====> Epoch: 95 Average loss: 1.5382\n",
      "Train Epoch: 96 [0/27800 (0%)]\tLoss: 1.541534\n",
      "Train Epoch: 96 [12800/27800 (46%)]\tLoss: 1.568942\n",
      "Train Epoch: 96 [25600/27800 (92%)]\tLoss: 1.527584\n",
      "====> Epoch: 96 Average loss: 1.5336\n",
      "Train Epoch: 97 [0/27800 (0%)]\tLoss: 1.504190\n",
      "Train Epoch: 97 [12800/27800 (46%)]\tLoss: 1.557215\n",
      "Train Epoch: 97 [25600/27800 (92%)]\tLoss: 1.416755\n",
      "====> Epoch: 97 Average loss: 1.5369\n",
      "Train Epoch: 98 [0/27800 (0%)]\tLoss: 1.522114\n",
      "Train Epoch: 98 [12800/27800 (46%)]\tLoss: 1.521587\n",
      "Train Epoch: 98 [25600/27800 (92%)]\tLoss: 1.542887\n",
      "====> Epoch: 98 Average loss: 1.5350\n",
      "Train Epoch: 99 [0/27800 (0%)]\tLoss: 1.421306\n",
      "Train Epoch: 99 [12800/27800 (46%)]\tLoss: 1.505316\n",
      "Train Epoch: 99 [25600/27800 (92%)]\tLoss: 1.598454\n",
      "====> Epoch: 99 Average loss: 1.5366\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(loader):\n",
    "            \n",
    "        img, _ = data\n",
    "        img = img.view(img.size(0), -1)\n",
    "        img = Variable(img)\n",
    "        if torch.cuda.is_available():\n",
    "            img = img.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(img)\n",
    "        loss = loss_function(recon_batch, img, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch,\n",
    "                batch_idx * len(img),\n",
    "                len(loader.dataset), 100. * batch_idx / len(loader),\n",
    "                loss.data / len(img)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "        epoch, train_loss / len(loader.dataset)))\n",
    "        # if epoch % 10 == 0:\n",
    "        #     save = to_img(recon_batch.cpu().data)\n",
    "        #     save_image(save, './vae_img/image_{}.png'.format(epoch))\n",
    "\n",
    "    torch.save(model.state_dict(), model_file)\n",
    "        \n",
    "    model.eval()\n",
    "    # Defining the Plot Style\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    # Plotting the last 100 values\n",
    "    plt.plot(losses[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e4bcf2-d1e6-418a-8c3c-1c709e6ce01b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82970bd4-62d8-4092-8037-d6b509e7654d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d962e5-69d9-44dd-b03d-e335702395a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
